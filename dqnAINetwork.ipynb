{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b18ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import keyboard\n",
    "from collections import deque\n",
    "from image_processing.screen_capture import screen_capture\n",
    "from image_processing.player_hp_detector import L_player_HP, R_player_HP\n",
    "from characters.ken import Ken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b48a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model architecture - using convolutional layers for image input\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # Convolutional layers for processing game screen images\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate the size of features after convolutions\n",
    "        conv_output_size = self._get_conv_output_size(input_channels, input_height, input_width)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "    def _get_conv_output_size(self, input_channels, height, width):\n",
    "        # Helper function to calculate output size after convolutions\n",
    "        dummy_input = torch.zeros(1, input_channels, height, width)\n",
    "        x = F.relu(self.conv1(dummy_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return int(np.prod(x.size()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, height, width)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # No activation on output layer (for Q-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56421b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_channels, input_height, input_width, num_actions, \n",
    "                 lr=0.0001, gamma=0.99, epsilon=1.0, epsilon_min=0.1, \n",
    "                 epsilon_decay=0.995, buffer_size=10000, batch_size=32):\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Q networks\n",
    "        self.policy_net = DQN(input_channels, input_height, input_width, num_actions)\n",
    "        self.target_net = DQN(input_channels, input_height, input_width, num_actions)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Set device (GPU if available)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net.to(self.device)\n",
    "        self.target_net.to(self.device)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.num_actions)\n",
    "        \n",
    "        # Convert state to PyTorch tensor\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "        return q_values.max(1)[1].item()\n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Compute Q(s_t, a)\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute V(s_{t+1})\n",
    "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
    "        \n",
    "        # Compute expected Q values\n",
    "        expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(q_values.squeeze(), expected_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients (optional)\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Street Fighter Environment wrapper\n",
    "class StreetFighterEnv:\n",
    "    def __init__(self, screen_region=(0, 0, 1280, 720), attack_mode='modern'):\n",
    "        self.screen_region = screen_region\n",
    "        self.character = Ken(attack_mode=attack_mode)\n",
    "        self.last_left_hp = 100\n",
    "        self.last_right_hp = 100\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 1000  # Maximum steps per episode\n",
    "        \n",
    "        # Movement states\n",
    "        self.moving_left = False\n",
    "        self.moving_right = False\n",
    "        self.crouching = False\n",
    "        \n",
    "        # Define actions (map integers to game actions)\n",
    "        self.actions = [\n",
    "            self._no_op,          # 0: Do nothing\n",
    "            self.character.light,  # 1: Light attack\n",
    "            self.character.medium, # 2: Medium attack\n",
    "            self.character.heavy,  # 3: Heavy attack\n",
    "            self.character.hadouken, # 4: Hadouken\n",
    "            self.character.shoryuken, # 5: Shoryuken\n",
    "            self.character.dragonlash_kick, # 6: Dragonlash kick\n",
    "            self._defense,         # 7: Defense/parry\n",
    "            self._burst,           # 8: Power burst\n",
    "            self._move_left,       # 9: Move left\n",
    "            self._move_right,      # 10: Move right\n",
    "            self._crouch,          # 11: Crouch\n",
    "            self._stop_movement    # 12: Stop all movement\n",
    "        ]\n",
    "        \n",
    "    def _no_op(self):\n",
    "        # Do nothing action\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    def _defense(self):\n",
    "        if self.character.attack_mode == 'modern':\n",
    "            self.character.impl.drive_parry()\n",
    "        else:\n",
    "            self.character.impl.parry()\n",
    "            \n",
    "    def _burst(self):\n",
    "        if self.character.attack_mode == 'modern':\n",
    "            self.character.impl.drive_impact()\n",
    "        else:\n",
    "            self.character.impl.burst()\n",
    "    \n",
    "    def _move_left(self):\n",
    "        # First stop any existing movement\n",
    "        self._stop_movement()\n",
    "        # Then start moving left\n",
    "        self.character.impl.hold_left()\n",
    "        self.moving_left = True\n",
    "        time.sleep(0.2)  # Brief movement\n",
    "        \n",
    "    def _move_right(self):\n",
    "        # First stop any existing movement\n",
    "        self._stop_movement()\n",
    "        # Then start moving right\n",
    "        self.character.impl.hold_right()\n",
    "        self.moving_right = True\n",
    "        time.sleep(0.2)  # Brief movement\n",
    "        \n",
    "    def _crouch(self):\n",
    "        # First stop any existing movement\n",
    "        self._stop_movement()\n",
    "        # Then start crouching\n",
    "        self.character.impl.hold_crouch()\n",
    "        self.crouching = True\n",
    "        time.sleep(0.2)  # Brief crouch\n",
    "    \n",
    "    def _stop_movement(self):\n",
    "        # Release all movement controls\n",
    "        if self.moving_left:\n",
    "            self.character.impl.release_left()\n",
    "            self.moving_left = False\n",
    "            \n",
    "        if self.moving_right:\n",
    "            self.character.impl.release_right()\n",
    "            self.moving_right = False\n",
    "            \n",
    "        if self.crouching:\n",
    "            self.character.impl.release_crouch()\n",
    "            self.crouching = False\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the game (you might need to implement a way to restart the match)\n",
    "        # For now, we'll just wait a bit and assume the game is reset\n",
    "        self._stop_movement()  # Make sure to release all movement keys\n",
    "        time.sleep(2)\n",
    "        self.step_count = 0\n",
    "        self.last_left_hp = 100\n",
    "        self.last_right_hp = 100\n",
    "        \n",
    "        # Get initial state\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Capture game screen\n",
    "        screen = screen_capture(self.screen_region)\n",
    "        \n",
    "        # Process the image for the neural network\n",
    "        # Convert to grayscale to reduce complexity\n",
    "        gray = cv2.cvtColor(screen, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Stack 4 frames for temporal information (optional, useful for detecting motion)\n",
    "        # For simplicity, we're using just one frame here\n",
    "        \n",
    "        # Resize to manageable size\n",
    "        resized = cv2.resize(gray, (84, 84))\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        normalized = resized / 255.0\n",
    "        \n",
    "        # Add channel dimension (DQN expects inputs in format [batch, channels, height, width])\n",
    "        state = np.expand_dims(normalized, axis=0)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        # Execute the selected action\n",
    "        if 0 <= action_idx < len(self.actions):\n",
    "            self.actions[action_idx]()\n",
    "        \n",
    "        # Small delay to let the game process the action\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # Get new state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        # Check if episode is done\n",
    "        self.step_count += 1\n",
    "        done = self._is_done()\n",
    "        \n",
    "        # Return step information\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        # Get current HP values\n",
    "        player_HP_area = screen_capture((115, 85, 1165, 100))\n",
    "        current_left_hp = L_player_HP(player_HP_area)\n",
    "        current_right_hp = L_player_HP(player_HP_area)\n",
    "        \n",
    "        # Calculate HP changes\n",
    "        left_hp_change = current_left_hp - self.last_left_hp\n",
    "        right_hp_change = current_right_hp - self.last_right_hp\n",
    "        \n",
    "        # Assuming player is on the left\n",
    "        reward = 0\n",
    "        \n",
    "        # Reward for dealing damage to opponent\n",
    "        if right_hp_change < 0:\n",
    "            reward += abs(right_hp_change) * 1.0  # More reward for dealing damage\n",
    "        \n",
    "        # Penalty for taking damage\n",
    "        if left_hp_change < 0:\n",
    "            reward -= abs(left_hp_change) * 0.8\n",
    "            \n",
    "        # Small penalty for time passing (encourages faster action)\n",
    "        reward -= 0.1\n",
    "        \n",
    "        # Update last HP values\n",
    "        self.last_left_hp = current_left_hp\n",
    "        self.last_right_hp = current_right_hp\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        # Episode ends if either player reaches 0 HP or max steps reached\n",
    "        player_HP_area = screen_capture((115, 85, 1165, 100))\n",
    "        left_hp = L_player_HP(player_HP_area)\n",
    "        right_hp = R_player_HP(player_HP_area)\n",
    "        \n",
    "        return left_hp <= 0 or right_hp <= 0 or self.step_count >= self.max_steps\n",
    "    \n",
    "    def close(self):\n",
    "        # Make sure to release all keys when closing the environment\n",
    "        self._stop_movement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4544fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_sf_agent(episodes=1000, target_update=10, save_interval=50):\n",
    "    # Define environment and agent parameters\n",
    "    env = StreetFighterEnv(screen_region=(0, 0, 1280, 720), attack_mode='modern')\n",
    "    input_channels = 1  # Grayscale image has 1 channel\n",
    "    input_height = 84\n",
    "    input_width = 84\n",
    "    num_actions = len(env.actions)\n",
    "    \n",
    "    agent = DQNAgent(\n",
    "        input_channels=input_channels,\n",
    "        input_height=input_height,\n",
    "        input_width=input_width,\n",
    "        num_actions=num_actions,\n",
    "        lr=0.0001,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_min=0.1,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Optional: Load pretrained model if available\n",
    "    try:\n",
    "        agent.load(\"street_fighter_dqn_latest.pth\")\n",
    "        print(\"Loaded pretrained model\")\n",
    "    except:\n",
    "        print(\"Training new model from scratch\")\n",
    "    \n",
    "    try:\n",
    "        # Training loop\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # Run one episode\n",
    "            while not done:\n",
    "                # Check for manual interrupt\n",
    "                if keyboard.is_pressed('q'):\n",
    "                    print(\"\\nTraining manually interrupted\")\n",
    "                    env.close()\n",
    "                    return agent\n",
    "                    \n",
    "                # Select and perform action\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # Store transition in replay buffer\n",
    "                agent.memory.push(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Train the network\n",
    "                agent.train()\n",
    "                \n",
    "                # Move to next state\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "            # Update target network periodically\n",
    "            if episode % target_update == 0:\n",
    "                agent.update_target_network()\n",
    "                \n",
    "            # Print episode stats\n",
    "            print(f\"Episode: {episode}, Total Reward: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if episode % save_interval == 0:\n",
    "                agent.save(f\"street_fighter_dqn_episode_{episode}.pth\")\n",
    "                agent.save(\"street_fighter_dqn_latest.pth\")  # Always save latest\n",
    "    \n",
    "    finally:\n",
    "        # Make sure to release all keys when training ends\n",
    "        env.close()\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1831f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the trained agent\n",
    "def test_agent(agent, episodes=5):\n",
    "    env = StreetFighterEnv(screen_region=(0, 0, 1280, 720), attack_mode='modern')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Always choose best action during testing (no exploration)\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = agent.policy_net(state_tensor)\n",
    "                action = q_values.max(1)[1].item()\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Check for manual interrupt\n",
    "                if keyboard.is_pressed('q'):\n",
    "                    print(\"\\nTesting manually interrupted\")\n",
    "                    return\n",
    "            \n",
    "            print(f\"Test Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    finally:\n",
    "        # Make sure to release all keys when testing ends\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f90de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting DQN training for Street Fighter...\")\n",
    "    print(\"Press 'q' at any time to stop training\")\n",
    "    time.sleep(3)\n",
    "    # Train agent\n",
    "    agent = train_sf_agent(episodes=500, target_update=10, save_interval=50)\n",
    "    \n",
    "    # Test trained agent\n",
    "    # print(\"\\nTesting trained agent...\")\n",
    "    # test_agent(agent, episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StreetFighterAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
